{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All rights reserved, 2021, By Youn-Sik Hong. 수업 목적으로만 활용 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고 서적\n",
    "    - Python Machine Learning(3rd Ed.), Sebastian Raschka , Vahid Mirjalili, Packt, 2019.10.\n",
    "        - 8장. Applying Machine Learning to Sentiment Analysis 코드 참조.\n",
    "    - 텐서플로2와 머신러닝으로 시작하는 자연어 처리, 전창욱, 최태균, 조중현, 신성진 지음, 위키북스, 2020.\n",
    "        - 4장. 텍스트 분류 예제 참조."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "train_file = DATA_IN_PATH + 'ratings_train.txt'\n",
    "train_data = pd.read_csv(train_file, header=0, delimiter='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현(re)을 사용해 아래에 해당하지 않는 기호는 모두 제거\n",
    "# 한글음절 :음절 11,174자 ('가'-'힣'), 자음('ㄱ'-'ㅎ'), 모음('ㅏ'-'ㅣ'), whitespace char(\\s)\n",
    "print(train_data['document'][0])\n",
    "review_text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]', '', train_data['document'][0])\n",
    "print(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(review_text)\n",
    "okt = Okt()\n",
    "review_text = okt.morphs(review_text, stem=True)\n",
    "print(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./kr_stopwords.txt', encoding='utf8') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [x.strip() for x in stopwords]    \n",
    "print(stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(review_text)\n",
    "revised_text = [w for w in review_text if len(w) > 1]\n",
    "clean_review = [w for w in revised_text if not w in stopwords]\n",
    "print(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(review, remove_stopwords, stop_words):\n",
    "    review_text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]', '', review)\n",
    "    word_review = okt.morphs(review_text, stem=True)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        revised_text = [w for w in word_review if len(w) > 1]\n",
    "        word_review = [token for token in revised_text \n",
    "                       if not token in stop_words]\n",
    "        \n",
    "    return word_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_review = []\n",
    "\n",
    "i = 0\n",
    "max = len(train_data['document'])\n",
    "for review in train_data['document']:\n",
    "    if (i % 1500 == 0):\n",
    "        print('진행률= %d 퍼센트' % ((i/max * 100)+1))\n",
    "    if type(review) == str:\n",
    "        clean_train_review.append(preprocessing(review, True, stopwords))\n",
    "    else:\n",
    "        clean_train_review.append([])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_review[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(clean_train_review)\n",
    "len(train_data['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = DATA_IN_PATH + 'ratings_test.txt'\n",
    "test_data = pd.read_csv(test_file, header=0, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "clean_test_review = []\n",
    "\n",
    "i = 0\n",
    "max = len(test_data['document'])\n",
    "for review in test_data['document']:\n",
    "    if i % 500 == 0:\n",
    "        print('진행률= %d 퍼센트' % ((i/max * 100)+1))\n",
    "        \n",
    "    if type(review) == str:\n",
    "        clean_test_review.append(preprocessing(review,True, stopwords))\n",
    "    else:\n",
    "        clean_test_review.append([])\n",
    "    i += 1                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_test_review)\n",
    "#len(clean_test_review[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow를 사용해 기계학습모델에 적용하기 위해서는 단어를 그대로 사용할 수 없으며,\n",
    "텍스트 데이터인 단어를 수치 데이터로 변환해야 함.\n",
    "따라서, text_to_sequences 라이브러리를 사용하여 전처리가 끝난 \n",
    "train_review와 test_review의 각 벡터를 index로 구성된 벡터로 변환.\n",
    "모든 index는 word_vocab에 저장되어 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train_review)\n",
    "train_sequence = tokenizer.texts_to_sequences(clean_train_review)\n",
    "test_sequence = tokenizer.texts_to_sequences(clean_test_review)\n",
    "\n",
    "word_vocab = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_train_review[0])\n",
    "print(train_sequence[0])\n",
    "#print(word_vocab)\n",
    "print(len(word_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 벡터는 서로 길이가 다름. 이 길이를 하나로 통일해야 기계학습모델에 적용할 수 있음.\n",
    "최대 길이(MAX_SEQUENCE_LENGTH=8)를 정하고, 이 길이보다 긴 벡터는 자르며,\n",
    "이 길이보다 짧은 벡터는 빈 자리에 0을 추가(padding)한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUNCE_LENGTH = 8\n",
    "\n",
    "train_inputs = pad_sequences(train_sequence, maxlen=MAX_SEQUNCE_LENGTH, padding='post')\n",
    "train_labels = np.array(train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_inputs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = pad_sequences(test_sequence, maxlen=MAX_SEQUNCE_LENGTH, padding='post')\n",
    "test_labels = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "TRAIN_INPUT_DATA = 'nsmc_train_input.npy'\n",
    "TRAIN_LABEL_DATA = 'nsmc_train_label.npy'\n",
    "TEST_INPUT_DATA = 'nsmc_test_input.npy'\n",
    "TEST_LABEL_DATA = 'nsmc_test_label.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = {}\n",
    "\n",
    "data_configs['vocab'] = word_vocab\n",
    "data_configs['vocab_size'] = len(word_vocab)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(DATA_IN_PATH):\n",
    "    ok.makedirs(DATA_IN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'wb'), train_inputs)\n",
    "np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), train_labels)\n",
    "np.save(open(DATA_IN_PATH + TEST_INPUT_DATA, 'wb'), test_inputs)\n",
    "np.save(open(DATA_IN_PATH + TEST_LABEL_DATA, 'wb'), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "clean_train_df = pd.DataFrame({'review':clean_train_review, \n",
    "                              'sentiment':train_data['label']})\n",
    "clean_test_df = pd.DataFrame({'review':clean_test_review, \n",
    "                              'sentiment':test_data['label']})\n",
    "clean_train_df.to_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA, index=False)\n",
    "clean_test_df.to_csv(DATA_IN_PATH + TEST_CLEAN_DATA, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
