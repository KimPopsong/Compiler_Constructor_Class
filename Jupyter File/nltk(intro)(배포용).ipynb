{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All rights reserved, 2021, By Youn-Sik Hong. 수업 목적으로만 활용 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고 사이트 \n",
    "    - nltk book 1. Language Processing and Python(https://www.nltk.org/book/ch01.html) 내용을 참고해서 자료를 만듦. \n",
    "    - nltk book의 1장 예제 일부 인용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새 창(window)이 나타나면, 설치 상태를 확인하고 **Download**를 눌러 설치하세요. 설치에 시간이 조금 걸립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자연어 처리를 위해서는 다양한 유형의 말뭉치(dataset)가 필요합니다. \n",
    "- nltk.book의 9개 말뭉치 중에는 소설이 4권이나 있지만, 성경, 신문기사, 채팅, 대통령 취임 연설문 등 다양합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9권의 샘플 말뭉치는 text1, text2, ..., text9로 참조할 수 있습니다. 정해진 변수 이름이니 그대로 사용하세요~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1  #말뭉치 제목을 알 수 있습니다.\n",
    "#text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 기능이 어떤 것이 있는지 맛 좀 볼까요?\n",
    "    - **concordance, similar, common_contexts, dispersion_plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **concordance** : 단어 검색. 단어가 나타난 문장까지 함께 출력합니다.\n",
    "백경(moby dick)은 포경선의 선장과 선원들이 거대한 고래를 포획하는 내용을 다룬 어드벤처 소설입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance('monstrous') #concordance - 일치, monstrous - 거대한, 괴이한"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 제인 오스틴은 오만과 편견(Pride and Prejudice)의 작가로 유명하죠. 최근 영화로도 만들어졌습니다.\n",
    "- Moby dick과 달리 Sense and Sensibility의 내용은 감성적이겠죠. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text2.concordance('affection')  #affection - a gentle feeling of fondness or liking(좋아하는 감정, 연민)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 같은 단어이지만, 소설 내용에 따라 전혀 다른 의미로 사용됨을 확인할 수 있습니다.\n",
    "- Moby dick에서 monstrous는 거대한, 괴이한과 같은 의미이지만, Sense and Sensibility에서는 매우, 너무(very)의 의미로 사용되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text1.concordance('affection')\n",
    "#text2.concordance('monstrous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 말뭉치에서는 어떤 단어가 주로 사용되었을까 생각하면서 아래 코드를 실행시켜 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text3.concordance('lived')  #성경 창세기\n",
    "#text4.concordance('nation') #대통령 취임 연설문(1798년) - terror, god 으로 바꿔보자\n",
    "#text5.concordance('lol')  #채팅 대화 내용 - im, ur로 바꿔보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **similar** : 지정한 단어와 문맥에서 나타난 유형과 비슷한 단어를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- similar란 메소드를 \"*지정한 단어(monstrous)와 의미가 비슷한 단어를 찾아주겠지*\"로 여러분이 짐작했다면 전혀 잘못 생각하고 있는 겁니다.\n",
    "- 기계학습을 자연어 처리에 적용할 때 여러분이 반드시 기억해야 할 것은 \"**AI는 단어가 무슨 뜻인지 모르고 있다**\"입니다.\n",
    "    - *그럼 monstrous와 비슷한 단어를 어떻게 찾을 수 있어요?* \n",
    "        - AI는 monstrous와 이웃(왼쪽, 오른쪽)에 있는 단어를 기억하고, 이와 비슷한 형태의 단어가 있는지를 비교해 찾습니다. \n",
    "        - 예를 들면 monstrous가 **most ___ size, the ___ picture** 에 나타났었는데, 이런 유형의 단어을 찾는 겁니다.\n",
    "- similiar 메소드는 이런 방식으로 monstrous와 비슷한 단어를 찾습니다. \n",
    "- similar의 출력 결과를 보면 '*이런 단어가 monstrous가 비슷하다고, 어이가 없네*'할 겁니다. \n",
    "    - 그렇지만 자세히 들여다 보면 비슷한 단어도 꽤 있습니다. 이게 AI가 갖고 있는 잠재력이죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.similar('monstrous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text1.concordance('curious')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sense and Sensibility에서 similar의 출력 결과는 꽤 그럴 듯 하지 않나요? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.similar('monstrous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **common_contexts** : 2개 이상의 단어가 공유하는 문맥(contexts)을 찾음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- similar 메소드의 의미를 이해했다면, common_contexts의 기능도 이와 비슷합니다.\n",
    "- 2개 이상의 단어는 리스트 ['monstrous','very'] 로 표현합니다. \n",
    "- common_contexts 메소드를 실행하기 전에 단어 각각에 대해 concordance 메소드부터 실행해 보세요.\n",
    "    - *monstrous*는 **a ___ prettry, am ___ pretty**, ...형태로 나타나왔죠.\n",
    "    - *very*는 **a ___ advanced, is ___ true, be ___ strange**, ... 형태로 나타났습니다.\n",
    "    - 그 결과를 보면 common_contexts의 실행 결과가 왜 그렇게 나왔는지 짐작할 수 있지 않나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text2.concordance('monstrous')\n",
    "#text2.concordance('very')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.common_contexts(['monstrous','very'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **dispersion_plot** : 단어 발생 분포를 출력해 줍니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dispersion은 분산(퍼뜨림)이란 뜻입니다.새가 씨앗을 (넓은 지역) 여기저기 퍼뜨린다고 할 때 사용합니다.\n",
    "- 단어가 발생한 지점이 x축(word offset)입니다. 원점에서 멀어질수록 문장 후반부를 가리킵니다.\n",
    "- text4는 미국 초대대통령의 취임 연설문입니다. \n",
    "    - citizen은 처음부터 끝까지 일정하게 나타나지만, \n",
    "    - America, freedom은 연설 끝부분에 집중적으로 등장함을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.dispersion_plot(['citizens', 'democracy', 'freedom', 'duties', 'America'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어휘 다양성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 말뭉치에 대한 간단한 통계 분석을 해 보겠습니다.\n",
    "    - nltk의 라이브러리에 있는 메소드만 사용하니까, 패키지 관광 온 기분이죠.\n",
    "        - 내가 가고 싶은 데로 가지 못하니 답답하죠.\n",
    "    - 라이브러리에 지나치게 의존하면 종속됩니다. \n",
    "        - 스스로 코딩해 보면서 나만의 독자 메소드를 만드는 게 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text3)) #text3: 성서의 창세기\n",
    "print(text3[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어휘 다양성(lexical richness)이란 중복되지 않은 단어를 얼마나 많이 사용했는가 입니다.\n",
    "- 파이썬의 set(집합) 타입으로 변환하면 중복된 단어를 쉽게 없앨 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tmp3 = sorted(set(text3))\n",
    "print(len(text3), len(tmp3))\n",
    "print('lexical richness = {}%'.format(np.round(len(tmp3)/len(text3)*100, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어휘 다양성을 구할 때 구둣점(punctuation)은 제외해야 겠죠. \n",
    "- tmp3 리스트는 오름차순 정렬된 상태이므로 구둣점 기호는 앞부분에 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tmp3[:15]) #구둣점 기호는 모두 13개입니다.\n",
    "punct_list = tmp3[:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp32 = [x for x in tmp3 if x not in punct_list]\n",
    "print(len(tmp3), ' >>> after removing punctuations >>>', len(tmp32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt32 = [x for x in text3 if x not in punct_list]\n",
    "print(len(text3), ' >>> after removing punctuations >>>', len(txt32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구둣점 기호를 없애기 전보다 약 1% 어휘 다양성이 증가했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lexical richness = {}%'.format(np.round(len(tmp32)/len(txt32)*100, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파이썬 string에서 제공하는 **punctuation** 리스트를 사용하면 간단하지 않을까요? \n",
    "    - **punctuation** 리스트에 포함된 구둣점 기호가 더 많지만, 기호 한 개로 구성되어 있습니다.\n",
    "    - 그러나 text3의 구둣점 기호는 2개 기호로 이루어진 경우도 포함되어 있습니다.\n",
    "    - 원시 데이터(raw data)의 특성을 먼저 살펴보는 게 중요하죠...\n",
    "    - **punctuation** 리스트는 표준화된 구둣점 기호들을 묶어 놓은 겁니다. \n",
    "    - 자연어에서는 구둣점을 일정하게 표현하지 않고 자유롭게(?) 표현하기 때문에 원시 데이터를 들여다 봐야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)\n",
    "print(type(string.punctuation))\n",
    "tmp322 = [x for x in tmp3 if x not in string.punctuation]\n",
    "print(len(tmp3), ' >>> after removing punctuations >>>', len(tmp322))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 빈도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count 메소드를 사용하면 어떤 단어가 몇 번 발생했는지 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3.count('Abel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(txt):\n",
    "    print(txt, np.round(len(set(txt))/len(txt), 5))\n",
    "\n",
    "def percentage(w, txt):\n",
    "    count = txt.count(w)\n",
    "    total = len(txt)\n",
    "    print('문자=', w, '빈도=', np.round(100*count/total, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text6이 어휘 다양성이 가장 높고, text2가 가장 낮게 나왔군요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_diversity(text1)\n",
    "lexical_diversity(text2)\n",
    "lexical_diversity(text3)\n",
    "lexical_diversity(text4)\n",
    "lexical_diversity(text6)\n",
    "lexical_diversity(text9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 발생 빈도가 높은 단어 중에는 관사(a, the), 구둣점 기호 등이 있습니다.\n",
    "- 자연어 처리 과정에서 이런 단어들을 불용어(stopwords)라고 부릅니다. \n",
    "- 전처리 과정에서 필수 작업 중 하나가 이런 불용어를 없애는 겁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage('a', text1)\n",
    "percentage('A', text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage('.', text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage(',', text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
