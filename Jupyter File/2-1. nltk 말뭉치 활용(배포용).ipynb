{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK 말뭉치 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All rights reserved, 2021, By Youn-Sik Hong. 수업 목적으로만 활용 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고 사이트 : https://www.nltk.org/book/ch02.html\n",
    "    - nltk book 2.Accessing Text Corpora and Lexical Resources 내용을 참고해서 자료를 만듦. \n",
    "    - nltk book 2장 예제 인용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gutenberg 말뭉치\n",
    "- NLTK는 Gutenberg 프로젝트(http://www.gutenberg.org) 의 25,000여개 전자책(무료) 중 18개의 텍스트를 말뭉치로 제공.\n",
    "- nltk\\.corpus\\.gutenberg 로 access.\n",
    "    - **fileids()** : 파일 이름을 리스트로 반환. 파일 이름은 '작가-소설.txt'로 구성.\n",
    "    - **raw('파일이름')** : 해당 말뭉치의 원문(raw text)을 반환.\n",
    "    - **sents('파일이름')** : 해당 말뭉치의 원문을 문장 단위로 반환.    \n",
    "    - **words('파일이름')** : 해당 말뭉치의 원문을 단어 단위로 반환.\n",
    "- raw-text: 자연어 처리 과정을 전혀 거치지 않은 원문(original text).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.corpus.gutenberg.fileids())\n",
    "print(len(nltk.corpus.gutenberg.fileids()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'austen-emma.txt'는 영국의 여류 소설가 Jane Austen의 작품 **Emma**(1815년)를 뜻함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(emma))\n",
    "print(len(emma))\n",
    "print(emma[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk.Text 클래스는 단어 단위로 구성된 객체를 생성. \n",
    "    - concordance, collocation, generate 등 다양한 기능을 제공.\n",
    "- 참고 사이트: \n",
    "    - nltk.Text 클래스: https://docs.huihoo.com/nltk/0.9.5/api/nltk.text.Text-class.html \n",
    "    - 파이썬으로 영어와 한국어 텍스트 다루기: https://www.lucypark.kr/courses/2015-dm/text-mining.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emma.tokens[:15])\n",
    "print(emma.count('Emma'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma.vocab() #returns frequency distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lines=5: 전체 37개 문장 중 5개만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma.concordance('surprize', lines=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gutenberg 말뭉치를 가져오기 위해 ...gutenberg.words()와 같이 길게 쓰는 대신 좀 더 편한 방법이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "gutenberg.fileids()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_raw = gutenberg.raw('austen-emma.txt')\n",
    "emma_words = gutenberg.words('austen-emma.txt')\n",
    "emma_sents = gutenberg.sents('austen-emma.txt')\n",
    "print(emma_raw[:20]) #원문\n",
    "print(emma_words[:10]) #단어 \n",
    "print(emma_sents[4:5]) #문장 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gutenberg 말뭉치(18개 텍스트) 통계 분석 \n",
    "- 통계 분석 내용\n",
    "    - average word length: 단어당 (평균) 알파벳 수\n",
    "    - average sentence length: 문장당 (평균) 단어 수\n",
    "    - lexical diversity: 전체 단어에서 중복되지 않은 단어 수\n",
    "- 분석 결과    \n",
    "    - 밀턴의 실락원(paradise)이 문장당 단어 수가 압도적으로 많음(52). 긴 문장으로 구성되었을 것으로 예측.\n",
    "    - 성서(bible)가 단어의 중복 사용 빈도가 가장 높으며(79), 윌리엄 블레이크(W. Blake)의 시(poem)가 가장 낮음(5).\n",
    "- 아래 통계에서 평균 단어 길이는 4,5가 아니라 3,4입니다. num_chars에 space도 포함되었기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid)) \n",
    "    num_words = len(gutenberg.words(fileid)) \n",
    "    num_sents = len(gutenberg.sents(fileid)) \n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid))) # 소문자로 통일 후, 중복되지 않는 단어 수를 카운트\n",
    "    print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "윌리엄 셰익스피어의 희곡 맥베스(Macbeth)를 선택해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt') \n",
    "print(len(macbeth_sentences), macbeth_sentences[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 맥베스(Macbeth)에서 가장 긴 문장은 무려 158개 단어로 된 문장이며, 딱 한 문장이 있습니다.\n",
    "- 문장당 평균 12단어인데, 평균보다 무려 13배 이상 길군요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_len = max(len(s) for s in macbeth_sentences)\n",
    "longest_sen = [s for s in macbeth_sentences if len(s) == longest_len]\n",
    "print(longest_len, len(longest_sen))\n",
    "print(longest_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  webtext\n",
    "- 여러 곳에서 수집한 다양한 종류의 (규모가 크지 않은) 말뭉치.\n",
    "    - Firefox discussion forum의 웹 텍스트, New York 시내에서 우연히 들은 대화 내용.\n",
    "    - 캐러비안의 해적(Pirates of the Carribean)의 대본, 개인 홍보 자료, 와인 품평."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "\n",
    "for fileid in webtext.fileids():\n",
    "    print(fileid, webtext.raw(fileid)[:65], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nps_chat\n",
    "- 채팅 말뭉치 : 미국 해군 대학원(the Naval Postgraduate School)에서 인터넷 악성 공격을 탐지하기 위한 연구 목적으로 구축\n",
    "- 10,000개 이상의 포스팅. 사용자 id는 \"UserNNN\"으로 바뀌었고, 개인 정보는 삭제함.\n",
    "- 말뭉치는 15개 파일로 구성됨. 각 파일별로 수백개의 포스팅이 있음.\n",
    "    - 파일은 포스팅 날짜 및 특정 연령대(10대, 20대, 30대, 40대, 50대 이상)로 구분.   \n",
    "- 파일 10-19-20s_706posts.xml 의미는 2006년 10월 19일 20대 채팅방에서 수집한 706개의 포스팅을 말함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import nps_chat\n",
    "\n",
    "chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n",
    "print(chatroom[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brown 말뭉치\n",
    "- Brown 말뭉치 매뉴얼 : http://icame.uib.no/brown/bcm.html\n",
    "- 미국 브라운 대학교(Brown University)에서 1961년에 최초로 만든 전자 말뭉치. \n",
    "- 500개 이상의 source로부터 수집한 텍스트로 구성했으며, 뉴스, 종교, 취미, 미스테리 등 다양한 장르(genre)로 구분."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "print(len(brown.categories()), len(brown.fileids())) #15개 카테고리, 500개 파일.\n",
    "print(brown.categories())\n",
    "print(brown.fileids()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.words(categories='news'))\n",
    "print(brown.words(fileids=['cg22']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.sents(categories=['news', 'editorial', 'reviews']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Brown 말뭉치는 장르간 구조적 차이(stylistics)를 비교하는 데 주로 사용합니다. \n",
    "    - 각 장르에서 법조동사(modal verb) 사용법을 비교해 보겠습니다. \n",
    "        - 법조동사는 가능성·허락·의도 등을 나타내는 조동사입니다.\n",
    "        - can/could, may/might, will/would, shall/should, must 등이 법조동사입니다.\n",
    "    - 법조동사(modal verb)는 일반 조동사(helping verb)와는 용법이 조금 다릅니다. \n",
    "        - do, be, have 등이 일반 조동사입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_text = brown.words(categories='news') #뉴스 카테고리\n",
    "fdist = nltk.FreqDist(w.lower() for w in news_text) #소문자 변환 -> 발생 빈도 계산."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- news 카테고리에서는 법조동사 중 will(389)이 발생 빈도가 압도적으로 높습니다.\n",
    "    - 또, wh 구문 중에는 who(268)에 이어 when(169)의 발생 빈도가 높게 나타났습니다.\n",
    "- 법조동사와 wh 구문 발생 빈도만으로 카테고리를 구분할 수 있지도 않을까요? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "whs = ['what', 'when', 'where', 'who', 'why']\n",
    "\n",
    "for m in modals:\n",
    "    print(m + ':', fdist[m], end=' ')  \n",
    "print()    \n",
    "for m in whs:\n",
    "    print(m + ':', fdist[m], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- religion 카테고리에서는 법조동사 중 can(84), may(79) 등이 발생 빈도가 비교적 높게 나타났습니다.\n",
    "    - 반면, wh 구문 중에는 who(102)에 이어 what(86), when(68)의 발생 빈도가 높게 나타났습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_text = brown.words(categories='religion')\n",
    "fdist = nltk.FreqDist(w.lower() for w in religion_text)\n",
    "\n",
    "#modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "#whs = ['what', 'when', 'where', 'who', 'why']\n",
    "for m in modals:\n",
    "    print(m + ':', fdist[m], end=' ')\n",
    "print()    \n",
    "for m in whs:\n",
    "    print(m + ':', fdist[m], end=' ')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 조건부 빈도 분포(**ConditionalFreqDist**)를 사용하여 카테고리 별로 법조동사의 사용 빈도를 비교해 보겠습니다.\n",
    "    - news 카테고리에서는 조동사 'will'(389)이, romance 카테고리에서는 'could'의 빈도(193)가 가장 높게 나타났습니다. \n",
    "    - 특정 법조동사 발생 빈도 만으로도 카테고리를 충분히 분류할 수 있음을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "          (genre, word)\n",
    "          for genre in brown.categories()\n",
    "          for word in brown.words(categories=genre))\n",
    "\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters 말뭉치\n",
    "- 130만개 단어로 이루어진 10,788개 뉴스.\n",
    "- 뉴스는 90개 토픽(topic)으로 나누어지며, 각 토픽은 training set과 test set으로 나뉨.\n",
    "- training set과 test set으로 나눈 목적은 인공지능 알고리즘을 사용해 뉴스를 자동으로 구분하기 위함.\n",
    "    - training set은 7,769개, test set은 3,019개로 7:3 비율."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "print(len(reuters.fileids()))\n",
    "print(reuters.fileids()[:10])\n",
    "print(reuters.fileids()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test set = ', sum(1 for w in reuters.fileids() if w.startswith('test')))\n",
    "print('training set = ', sum(1 for w in reuters.fileids() if w.startswith('training')))\n",
    "print(reuters.fileids()[3011:3019])\n",
    "print(reuters.fileids()[3019:3026])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reuters.categories())\n",
    "print(len(reuters.categories()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Brown 말뭉치와 다른 점은 Reuters 말뭉치의 카테고리(토픽)들은 중복될 수 있다는 점입니다.\n",
    "    - 뉴스 특성상 일부 뉴스는 여러 개의 토픽을 커버하기 때문입니다. \n",
    "    - 어떤 토픽이 어느 뉴스에서 다뤄졌는지 검색을 통해 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reuter 말뭉치에서 한 개의 기사 또는 여러 개 기사에서 다뤘던 토픽을 찾을 수 있습니다.\n",
    "    - **categories()** parameter로 한 개의 기사나 여러 개 기사(리스트로 표시)를 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reuters.categories('training/9865'))\n",
    "print(reuters.categories('training/9880'))\n",
    "print(reuters.categories(['training/9865', 'training/9880']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reuter 말뭉치에서 한 개 토픽 또는 여러 개 토픽을 다룬 기사를 찾을 수 있습니다.\n",
    "    - **fileids()** parameter로 한 개의 토픽 또는 여러 개 토픽(리스트로 표시)을 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reuters.fileids('barley')[:5])\n",
    "print(reuters.fileids(['barley', 'corn'])[5:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파일 이름이나 카테고리에 속한 단어들(즉 문장)을 확인할 수 있습니다. \n",
    "    - **words()** parameter로 기사나 토픽을 지정할 수 있습니다.\n",
    "    - 맨 앞에 뉴스 제목이  나타나기 때문에 대문자로 출력됩니다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reuters.words('training/9865')[:10])\n",
    "print(reuters.words(['training/9865', 'training/9880']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reuters.words(categories='barley'))\n",
    "print(reuters.words(categories=['barley', 'corn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미국 대통령 취임연설(Inaugural Address) 말뭉치\n",
    "- 58개의 대통령 취임 연설로 구성되며, 4년단위 취임년도 정보를 갖고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "\n",
    "print(inaugural.fileids())\n",
    "print(len(inaugural.fileids()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일 이름은 '취임연도-대통령이름.txt' 형태이므로, 맨 앞 4개 숫자만 가져오면 취임연도를 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_list = [fileid[:4] for fileid in inaugural.fileids()]\n",
    "print(year_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조건부 빈도분석을 사용하여 연설문에서 'America'와 'citizen'이란 단어 빈도 수가 연도별로 어떻게 변화했는지 알아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "          (target, fileid[:4])\n",
    "          for fileid in inaugural.fileids()\n",
    "              for w in inaugural.words(fileid)\n",
    "                  for target in ['america', 'citizen']\n",
    "          if w.lower().startswith(target)) \n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK 말뭉치\n",
    "- 이미 소개한 말뭉치 외에도 품사(POS) 태그, 개체명, 문법 구조 등과 같은 분석 정보를 함께 제공하는 다양한 말뭉치들이 있습니다.\n",
    "- 아래 참고 사이트에 자세한 내용이 있으니 참고하기 바랍니다.\n",
    "    - NLTK 다운로드 및 설치  : http://nltk.org/data.\n",
    "    - NLTK 말뭉치 전체 리스트 : http://nltk.org/nltk_data/\n",
    "    - NLTK 말뭉치 사용 방법 : http://nltk.org/howto "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDHR 말뭉치\n",
    "- udhr(the Universal Declaration of Human Rights, 세계인권선언)은 300여개 언어로 된 말뭉치를 갖고 있습니다.\n",
    "- 영어권 이외 다양한 언어권 말뭉치를 제공합니다. \n",
    "    - 한글을 포함한 다문화권 말뭉치를 다루기 위해서는 파이썬 인코딩 방식을 알아야 합니다.\n",
    "    - udhr의 fileids에는 UTF8 이나 Latin1처럼 문자체계 인코딩 정보가 포함되어 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.corpus.cess_esp.words()) #the corpora in Spanish and Portuguese\n",
    "print(nltk.corpus.floresta.words()) #포르투갈어\n",
    "print(nltk.corpus.indian.words('hindi.pos')) #인도어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nltk.corpus.udhr.fileids()))\n",
    "print(nltk.corpus.udhr.fileids()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리말 파일을 찾으려면 어떻게 하면 될까요? 아래 2가지 방법 모두 가능하죠..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in nltk.corpus.udhr.fileids() if 'Korea' in w]\n",
    "#[w for w in nltk.corpus.udhr.fileids() if w.startswith('Korea')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 일본어 파일을 찾아볼까요? 파일이 3개씩이나 있어 느낌이 좀 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in nltk.corpus.udhr.fileids() if w.startswith('Japan')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.corpus.udhr.words('Korean_Hankuko-UTF8')[11:])\n",
    "print(nltk.corpus.udhr.words('Japanese_Nihongo-UTF8')[20:])\n",
    "print(nltk.corpus.udhr.words('Chinese_Mandarin-GB2312')[20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- udhr 말뭉치에 포함된 몇 개 언어에 대해 조건빈도분포를 사용하여 단어 길이를 비교해 보겠습니다.\n",
    "- Latin1과 utf8 코드체계 인코딩을 사용하는 언어 그룹끼리 비교해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 같은 Latin1 계 어원에서 파생된 English와 German_Deutsch가 단어 길이에서 차이가 납니다.\n",
    "    - Hungarian_Magyar와 German_Deutsch는 게르만 족이라 어원이 비슷해 보입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
    "     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "          (lang, len(word))\n",
    "          for lang in languages\n",
    "          for word in udhr.words(lang + '-Latin1'))\n",
    "cfd.plot(cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- utf-8 코딩체계를 사용하는 것 빼면 전혀 다른 어원을 갖는 언어들입니다.\n",
    "    - 일본어가 단어 길이가 제일 짧군요. \n",
    "    - 하와이 고유 언어의 단어 길이가 제일 긴 편입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['Punjabi_Panjabi', 'Hawaiian', 'Greek_Ellinika',\n",
    "     'Hebrew_Ivrit', 'Japanese_Nihongo', 'Korean_Hankuko']\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "          (lang, len(word))\n",
    "          for lang in languages\n",
    "          for word in udhr.words(lang + '-UTF8'))\n",
    "cfd.plot(cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = udhr.words('English-Latin1')\n",
    "fd = nltk.FreqDist(tokens)\n",
    "print(fd.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = udhr.words('Korean_Hankuko-UTF8')\n",
    "fd = nltk.FreqDist(tokens)\n",
    "print(fd.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 말뭉치 구조\n",
    "- 말뭉치 구조는 텍스트 집합(gutenberg), 카테고리 분할(brown), 카테고리 중복(reuters), 시간 구조(inaugural address) 등 다양.\n",
    "- 상세한 내용은 help(nltk.corpus.reader)를 실행하면 주피터 노트북에서도 볼 수 있습니다.\n",
    "    - 비추: 내용이 엄청 많아 내용 볼 때까지 시간이 꽤 걸립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
